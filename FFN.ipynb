{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Prepare Data\n",
    "Load training, validation, and test data from the data folder. Convert graph structures into suitable matrix representations for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shapes: X=(6695, 21, 2367), y=(6695, 21, 21)\n",
      "Validation shapes: X=(2231, 21, 2367), y=(2231, 21, 21)\n",
      "Test shapes: X=(2233, 21, 2367), y=(2233, 21, 21)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load data\n",
    "with open('data/train_graphs.dat', 'rb') as file:\n",
    "    train_graphs = pickle.load(file)\n",
    "with open('data/val_graphs.dat', 'rb') as file:\n",
    "    val_graphs = pickle.load(file)\n",
    "with open('data/test_graphs.dat', 'rb') as file:\n",
    "    test_graphs = pickle.load(file)\n",
    "\n",
    "# Combine all graphs for dimension analysis\n",
    "all_graphs = []\n",
    "all_graphs.extend(train_graphs)\n",
    "all_graphs.extend(val_graphs)\n",
    "all_graphs.extend(test_graphs)\n",
    "\n",
    "def get_max_dimensions(graphs):\n",
    "    max_parts = max(len(graph.get_parts()) for graph in graphs)\n",
    "    max_part_id = max(int(part.get_part_id()) for graph in graphs for part in graph.get_parts())\n",
    "    max_family_id = max(int(part.get_family_id()) for graph in graphs for part in graph.get_parts())\n",
    "    return max_parts, max_part_id, max_family_id\n",
    "\n",
    "max_parts, max_part_id, max_family_id = get_max_dimensions(all_graphs)\n",
    "\n",
    "# Initialize encoders with explicit categories\n",
    "part_categories = np.arange(max_part_id + 1)\n",
    "family_categories = np.arange(max_family_id + 1)\n",
    "\n",
    "part_encoder = OneHotEncoder(categories=[part_categories], sparse_output=False, handle_unknown='ignore')\n",
    "family_encoder = OneHotEncoder(categories=[family_categories], sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit encoders\n",
    "part_encoder.fit(part_categories.reshape(-1, 1))\n",
    "family_encoder.fit(family_categories.reshape(-1, 1))\n",
    "\n",
    "def process_dataset(graphs):\n",
    "    features_list = []\n",
    "    adj_matrices = []\n",
    "    \n",
    "    for graph in graphs:\n",
    "        parts = list(graph.get_parts())\n",
    "        n_parts = len(parts)\n",
    "        \n",
    "        # Convert IDs to int explicitly\n",
    "        part_ids = np.array([int(part.get_part_id()) for part in parts]).reshape(-1, 1)\n",
    "        family_ids = np.array([int(part.get_family_id()) for part in parts]).reshape(-1, 1)\n",
    "        \n",
    "        part_encoded = part_encoder.transform(part_ids)\n",
    "        family_encoded = family_encoder.transform(family_ids)\n",
    "        \n",
    "        features = np.hstack([part_encoded, family_encoded])\n",
    "        \n",
    "        if n_parts < max_parts:\n",
    "            padding = np.zeros((max_parts - n_parts, features.shape[1]))\n",
    "            features = np.vstack([features, padding])\n",
    "            \n",
    "        adj_matrix = graph.get_adjacency_matrix(tuple(parts))\n",
    "        if n_parts < max_parts:\n",
    "            adj_matrix = np.pad(adj_matrix, ((0, max_parts - n_parts), (0, max_parts - n_parts)))\n",
    "            \n",
    "        features_list.append(features)\n",
    "        adj_matrices.append(adj_matrix)\n",
    "    \n",
    "    return np.array(features_list), np.array(adj_matrices)\n",
    "\n",
    "# Process datasets\n",
    "X_train, y_train = process_dataset(train_graphs)\n",
    "X_val, y_val = process_dataset(val_graphs)\n",
    "X_test, y_test = process_dataset(test_graphs)\n",
    "\n",
    "print(f\"Training shapes: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Validation shapes: X={X_val.shape}, y={y_val.shape}\")\n",
    "print(f\"Test shapes: X={X_test.shape}, y={y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Neural Network Model\n",
    "Create a feed-forward neural network using PyTorch with appropriate input size (based on one-hot encoding), hidden layers, and output size (adjacency matrix prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers(x)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Convert data to tensors\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m X_train_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mFloatTensor(X_train)\n\u001b[1;32m     30\u001b[0m y_train_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(y_train\u001b[38;5;241m.\u001b[39mreshape(y_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Create data loader\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Prepare data dimensions\n",
    "batch_size = 32\n",
    "num_features = X_train.shape[1]  # Features per part\n",
    "num_parts = max_parts  # Maximum number of parts\n",
    "adj_matrix_size = num_parts * num_parts  # Size of flattened adjacency matrix\n",
    "\n",
    "# Model definition\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, num_features, num_parts, hidden_size=512):\n",
    "        super(FFN, self).__init__()\n",
    "        self.num_parts = num_parts\n",
    "        self.flatten = nn.Flatten(1)  # Flatten everything except batch dimension\n",
    "        \n",
    "        input_size = num_features * num_parts\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_parts * num_parts),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        return self.layers(x)\n",
    "\n",
    "# Convert data to tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train.reshape(y_train.shape[0], -1))\n",
    "\n",
    "# Create data loader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize model\n",
    "model = FFN(num_features, num_parts)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "Implement training loop with batch processing, loss calculation, and optimization. Include validation step to monitor model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the neural network model\n",
    "class GraphPredictionModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(GraphPredictionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = X_train.shape[2]  # Number of features after one-hot encoding\n",
    "hidden_size = 128\n",
    "output_size = y_train.shape[1] * y_train.shape[2]  # Flattened adjacency matrix\n",
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.reshape(y_train.shape[0], -1), dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.reshape(y_val.shape[0], -1), dtype=torch.float32)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = GraphPredictionModel(input_size, hidden_size, output_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss/len(val_loader):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "Evaluate model performance using the provided edge_accuracy metric and additional relevant metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "\n",
    "import torch\n",
    "\n",
    "# Convert test data to PyTorch tensors\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.reshape(y_test.shape[0], -1), dtype=torch.float32)\n",
    "\n",
    "# Create test data loader\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_edges = 0\n",
    "    total_edges = 0\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate edge accuracy\n",
    "            predicted_adj_matrix = outputs.round().reshape(y_batch.shape[0], y_train.shape[1], y_train.shape[2])\n",
    "            target_adj_matrix = y_batch.reshape(y_batch.shape[0], y_train.shape[1], y_train.shape[2])\n",
    "            correct_edges += (predicted_adj_matrix == target_adj_matrix).sum().item()\n",
    "            total_edges += y_batch.numel()\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    edge_accuracy = correct_edges / total_edges * 100\n",
    "    return avg_loss, edge_accuracy\n",
    "\n",
    "# Evaluate on test data\n",
    "test_loss, test_edge_accuracy = evaluate_model(model, test_loader)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Edge Accuracy: {test_edge_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction and Graph Construction\n",
    "Implement methods to convert model predictions back into Graph objects using the provided Graph class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert model predictions back into Graph objects\n",
    "def predictions_to_graphs(predictions, parts_list):\n",
    "    graphs = []\n",
    "    for i, adj_matrix in enumerate(predictions):\n",
    "        parts = parts_list[i]\n",
    "        graph = Graph()\n",
    "        for part in parts:\n",
    "            graph.__add_node(Node(part.get_part_id(), part))\n",
    "        for j, part1 in enumerate(parts):\n",
    "            for k, part2 in enumerate(parts):\n",
    "                if adj_matrix[j, k] == 1:\n",
    "                    graph.add_undirected_edge(part1, part2)\n",
    "        graphs.append(graph)\n",
    "    return graphs\n",
    "\n",
    "# Example usage\n",
    "predicted_adj_matrices = model(X_test_tensor).round().detach().numpy().reshape(X_test.shape[0], y_test.shape[1], y_test.shape[2])\n",
    "predicted_graphs = predictions_to_graphs(predicted_adj_matrices, [list(graph.get_parts()) for graph in test_graphs])\n",
    "\n",
    "# Display the first predicted graph\n",
    "predicted_graphs[0].draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model\n",
    "Save the trained model following the MyPredictionModel interface for later use in evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the trained model\n",
    "model_file_path = 'trained_graph_prediction_model.pkl'\n",
    "with open(model_file_path, 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "# Save the model class definition\n",
    "class MyTrainedModel(MyPredictionModel):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def predict_graph(self, parts: Set[Part]) -> Graph:\n",
    "        parts_encoded, _ = graph_to_matrix(Graph(parts))\n",
    "        parts_tensor = torch.tensor(parts_encoded, dtype=torch.float32).unsqueeze(0)\n",
    "        adj_matrix_pred = self.model(parts_tensor).round().detach().numpy().reshape(parts_encoded.shape[0], parts_encoded.shape[0])\n",
    "        return predictions_to_graphs([adj_matrix_pred], [list(parts)])[0]\n",
    "\n",
    "# Save the model following the MyPredictionModel interface\n",
    "trained_model = MyTrainedModel(model)\n",
    "with open('my_trained_model.pkl', 'wb') as file:\n",
    "    pickle.dump(trained_model, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
